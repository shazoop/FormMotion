{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.draw import random_shapes\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a dimxdim image of a random shape of a size between given bounds\n",
    "def shape_gen(imgDim, minimum,maximum):\n",
    "    x = random_shapes((dim, dim), max_shapes=1, multichannel=False, intensity_range=(0,0),min_size = minimum, max_size=maximum)[0]\n",
    "    x = (-1*x+255)/255\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create array of start images\n",
    "\n",
    "images_seq = [shape_gen(imgDim,10,25) for i in range(1000)]\n",
    "images_seq = np.array(images_seq)\n",
    "images_seq = np.expand_dims(images_seq,1)\n",
    "\n",
    "test_seq = [shape_gen(imgDim,10,25) for i in range(100)]\n",
    "test_seq = np.array(test_seq)\n",
    "test_seq = np.expand_dims(test_seq,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a given collection of start images with (n,1,k,k), with n images of dimension k by k. Trans_range is one-sided,\n",
    "#so trans_range = 4 allows translations of 4 up or down, 4 left or right. same for rot_center, how much the center of \n",
    "#rotation can shift to one-side both up/down. rot_angle is maximum angle in DEGREES\n",
    "def rand_trans(data, trans_range, rot_center, rot_angle): \n",
    "    n, row, col = data[:,0,:,:].shape\n",
    "    Index = [random.randint(0,1) for i in range(n)] #randomly choose either translation or rotation\n",
    "    \n",
    "    def M_gen(i):\n",
    "        if i == 0:\n",
    "            hor = random.randint(-trans_range, trans_range)\n",
    "            ver = random.randint(-trans_range, trans_range)\n",
    "            x = np.float32([[1,0,hor],[0,1,ver]]) #create translation matrix\n",
    "        else:\n",
    "            center = (row/2+random.randint(-rot_center,rot_center),col/2+random.randint(-rot_center,rot_center))\n",
    "            angle = random.uniform(0,rot_angle)\n",
    "            x = cv2.getRotationMatrix2D(center,angle,1) #create rotation matrix\n",
    "        return(x)\n",
    "    \n",
    "    Z = np.array([M_gen(i) for i in Index])\n",
    "    \n",
    "    return(Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = rand_trans(images_seq, 2, 2, 45)\n",
    "M2 = rand_trans(test_seq, 2, 2, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a given collection of start images - \"data\" - and their associated transformations -\"transform\", one for each start image-\n",
    "#will generate a movie sequence of the desired length. note \"data\" should be np.array of shape (n,1,k,k).\n",
    "def image_gen(data, transform, movie_len, imgDim):\n",
    "    X = data\n",
    "    for j in range(movie_len-1):\n",
    "        Y = np.array([cv2.warpAffine(X[i,j],M[i],(imgDim,imgDim)) for i in range(data.shape[0])])\n",
    "        Y = np.expand_dims(Y,1) #add the second axis, which denotes time, to concatenate the images\n",
    "        X = np.concatenate((X,Y),axis=1)\n",
    "    \n",
    "    X = X.reshape(X.shape[0],X.shape[1],X.shape[2]**2)\n",
    "    return(X) #reshape so it's easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = image_gen(images_seq, M, 10, imgDim)\n",
    "test_raw = image_gen(test_seq, M2, 10, imgDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 256)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gen(test_seq, testSize, maxShapes):\n",
    "    \n",
    "    #Set up first array, then just loop by concatenating.\n",
    "    \n",
    "    test_data = test_seq[np.random.randint(0,test_seq.shape[0],maxShapes)] #choose random number of movie sequences\n",
    "    test_data = np.sum(test_data,axis= 0) #sum them together\n",
    "    test_data[test_data > 1] = 1 #cap images at 1\n",
    "    test_data = np.expand_dims(test_data, axis = 0) #add back the extra first axis, which indexes every movie sequence\n",
    "    \n",
    "    for datum in range(1,testSize):\n",
    "        subset = test_seq[np.random.randint(0,test_seq.shape[0],maxShapes)]\n",
    "        subset = np.sum(subset,axis=0)\n",
    "        subset[subset > 1] = 1\n",
    "        subset = np.expand_dims(subset, axis = 0)\n",
    "        test_data = np.concatenate((test_data,subset),axis=0)\n",
    "    \n",
    "    return(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = multi_gen(test_raw, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10, 256)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rationale: since movies of multiple objects, with overlap, will just be sum of two images with values capped at 255,\n",
    "#want transforms to reflect this ceiling like behavior or else prediction might be bad. \n",
    "#Will use a smooth squashing function, tanh.\n",
    "\n",
    "def squash(x):\n",
    "    y = 1.3*np.tanh(x)\n",
    "    return(y)\n",
    "\n",
    "def squash_deriv(x):\n",
    "    y = 1.3*(1-np.tanh(x)**2)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTransforms = 15\n",
    "trainSteps = 1000\n",
    "batchSize = 20 #training \n",
    "sparsePen = 1\n",
    "derivPen = .5\n",
    "epsilon = .0001 #variance of normal desntiy in the smoothed bump function.\n",
    "eta = .01 #learning rate of phi, the dictionary of transforms\n",
    "eta2 = .05 #learning rate of beta, the transform coeff\n",
    "eta3 = .1 #Learning rate of center\n",
    "eta4 = .1 #Learning rate of radii\n",
    "numInference = 50 #number of basis trans * length of movie sequence\n",
    "betaStep = 10 #FOR TRAINED TRANSFORMS. mini cycle for beta\n",
    "attStep = 5 #FOR TRAINED TRANSFORMS. mini cycle for center/radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = data[np.random.randint(0,data.shape[0],batchSize)]\n",
    "Input = data_batch[:,:9,:] #will predict next frame from these\n",
    "Output = data_batch[:,1:10,:] #the frames to be predicted\n",
    "dim = data_batch.shape[2]\n",
    "phi = np.random.rand(dim, dim ,numTransforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.zeros((Input.shape[0],Input.shape[1],phi.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta shape:(30, 9, 15)phi shape:(1024, 1024, 15)Input shape:(30, 9, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"beta shape:\" + str(beta.shape) + \"phi shape:\" + str(phi.shape) + \"Input shape:\" + str(Input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.einsum('btk,ijk,btj -> bti', beta, phi, Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = Output - squash(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 9, 1024)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "check1 = timeDiff(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1024, 15)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = learnTransform(Input, Output, phi, beta, eta, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 15)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training algorithm, image sequences with one shape. No need to infer center/radii yet.\n",
    "def timeDiff(beta):\n",
    "    #assuming beta has shape bkt, where k for trans, t for length, B for number of images (usually batch size)\n",
    "    \n",
    "    Zeroes = np.zeros((beta.shape[0],1,beta.shape[2]))\n",
    "    timeDeriv = np.diff(beta, axis= 1)\n",
    "    X = np.concatenate((Zeroes, timeDeriv), axis=1)\n",
    "    Y = np.concatenate((timeDeriv,Zeroes), axis =1)\n",
    "    Z = -1*np.sign(X)+np.sign(Y)\n",
    "    \n",
    "    return(Z)\n",
    "\n",
    "def inferBeta(Input, Output, phi, sparsePen, derivPen, eta2, numInference):\n",
    "    #Assume data has shape (B, T, n2), where number of images(usually batch size) , T is time, and n2 is n^2 for nxn image\n",
    "    #phi is of shape (i,i',k) where i,i' denote image dim, k denotes transform, T is time, and B is batch\n",
    "    #Input are first T-1 frames. Output are frames 2 to T. Infer i^th frame from previous.\n",
    "    \n",
    "    beta = np.zeros((Input.shape[0],Input.shape[1],phi.shape[2])) #initialize beta at 0. beta is shape btk\n",
    "    \n",
    "    for step in range(numInference):\n",
    "        predictions = np.einsum('btk,ijk,btj -> bti', beta, phi, Input)\n",
    "        error = Output - predictions\n",
    "        dBeta = np.einsum('bti,ijk,btj -> btk', error, phi, Input) - sparsePen*np.sign(beta) - derivPen*timeDiff(beta)\n",
    "        beta += eta2*dBeta\n",
    "        \n",
    "    return(beta)\n",
    "    \n",
    "def learnTransform(Input, Output, phi, beta, eta, batchSize, numInference):\n",
    "    \n",
    "    for step in range(numInference):\n",
    "        predictions = np.einsum('btk,ijk,btj -> bti', beta, phi, Input)\n",
    "        error = Output - predictions\n",
    "        dPhi = np.einsum('bti, btj, btk -> ijk', error, Input, beta)\n",
    "        phi += (eta/batchSize)*dPhi\n",
    "    \n",
    "    return(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning algo, image sequences with one shape.\n",
    "\n",
    "def Training1(dataset, trainSteps, batchSize, sparsePen, derivPen, eta, eta2, numInference, numTransforms):\n",
    "    \n",
    "    #intialize phi\n",
    "    dim = dataset.shape[2]\n",
    "    phi = np.random.rand(dim, dim ,numTransforms) #n2 x n2 x 15\n",
    "    \n",
    "    #start training\n",
    "    for trial in range(trainSteps):\n",
    "        \n",
    "        data_batch = dataset[np.random.randint(0,dataset.shape[0],batchSize)] #create random batch\n",
    "        Input = data_batch[:,:9,:] #will predict next frame from these\n",
    "        Output = data_batch[:,1:10,:] #the frames to be predicted\n",
    "        \n",
    "        #Find beta, fix, then optimize phi\n",
    "        beta = inferBeta(Input, Output, phi, sparsePen, derivPen, eta2, numInference)\n",
    "        phi = learnTransform(Input, Output, phi, beta, eta, batchSize, numInference)\n",
    "        \n",
    "        #renormalize by capping absolute value to 1\n",
    "        phi = phi/(np.abs(phi).max(axis=(0,1),keepdims=True))\n",
    "        \n",
    "    \n",
    "    return(phi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 15)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnedPhi = Training1(data, 1, 5, sparsePen, derivPen, eta, eta2, 1, numTransforms)\n",
    "learnedPhi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_seq = data[0]\n",
    "movie_seq.shape\n",
    "dim = int(np.sqrt(movie_seq.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = movie_seq[:(movie_seq.shape[0]-1),:]\n",
    "Output = movie_seq[1:,:]\n",
    "Input2 = Input.reshape((Input.shape[0],dim,dim)) #just reshape to make region of interest indices more natural\n",
    "movie_length = Input2.shape[0]\n",
    "numInference2 = 10 #this is arbitrary. want to update each of beta, c, r a little\n",
    "beta = np.zeros((Input2.shape[0],phi.shape[2])) #initialize beta at 0. beta is shape tk. movie_seq first dim is t\n",
    "center = (dim/2)*np.ones((2,Input2.shape[0],phi.shape[2])) #initialize centers at center of image. each center is 2-vector\n",
    "radius2 = 17*np.ones((Input2.shape[0],phi.shape[2])) #initialize radius2 at 17, so start off region of interest as entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalMatrix(movie_length, dim, center, radius2, epsilon):\n",
    "    normal_matrix = np.empty((center.shape[2], movie_length, dim,dim)) #shape is ktdd, where k is number of transforms, t is time, d is dim of image\n",
    "    \n",
    "    for k in range(center.shape[2]):\n",
    "        for t in range(movie_length):\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    normal_matrix[k,t,i,j] = (1/np.sqrt(2*np.pi*epsilon))*np.exp(-(radius2[t,k]-np.linalg.norm(np.array([i,j])-center[:,t,k])**2)**2/(2*epsilon**2))\n",
    "    \n",
    "    return(normal_matrix)\n",
    "\n",
    "def center_ParDeriv(movie_length, dim, center, radius2, epsilon): #calculates partial deriv for center, where each entry is 2-vector\n",
    "    normal_matrix = np.empty((center.shape[2], movie_length, dim,dim)) #shape is ktdd, where k is number of transforms, t is time, d is dim of image\n",
    "    \n",
    "    for k in range(center.shape[2]):\n",
    "        for t in range(movie_length):\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    normal_matrix[k,t,i,j] = (1/np.sqrt(2*np.pi*epsilon))*np.exp(-(radius2[t,k]-np.linalg.norm(np.array([i,j])-center[:,t,k])**2)**2/(2*epsilon**2))\n",
    "    \n",
    "    distance_pd = np.empty((2,center.shape[2], movie_length, dim, dim)) #shape 2ktdd\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            distance_pd[:,k,t,i,j] = 2*(np.array([i,j])-center[:,t,k])\n",
    "            \n",
    "    centerPD = np.multiply(distance_pd, normal_matrix)\n",
    "    return(centerPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15, 9, 1024)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center2 = center_ParDeriv(movie_length, dim, center, radius2, epsilon).reshape((2,center.shape[2],movie_length, Input.shape[1]))\n",
    "center2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localImage(Input2, movie_length, dim, center, radius2, epsilon):\n",
    "    local_matrix = np.empty((center.shape[2], movie_length, dim,dim)) #shape is ktdd, where k is number of transforms, t is time, d is dim of image\n",
    "    \n",
    "    for k in range(center.shape[2]):\n",
    "        for t in range(movie_length):\n",
    "            for i in range(dim):\n",
    "                for j in range(dim):\n",
    "                    local_matrix[k,t,i,j] = (np.linalg.norm(np.array([i,j])-center[:,t,k])**2)/radius2[t,k]\n",
    "    local_matrix[local_matrix <= 1+ epsilon] = 1\n",
    "    local_matrix[local_matrix > 1 + epsilon] = 0\n",
    "    \n",
    "    LocalInput2 = np.multiply(local_matrix, Input2)\n",
    "    \n",
    "    return(LocalInput2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 9, 32, 32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localImage(Input2, movie_length, dim, center, radius2, epsilon).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferRadius2(Input2, center, Output, radius2, phi, beta, eta3, epsilon, attStep):\n",
    "    \n",
    "    for step in range(attStep):\n",
    "    \n",
    "        localimg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1])) #calculate localized images and reshape\n",
    "        \n",
    "        predictions = np.einsum('tk,ijk,ktj -> ti', beta, phi, localimg)\n",
    "        error = Output - squash(predictions)\n",
    "        dTanh = squash_deriv(predictions)\n",
    "        ParDeriv = np.multiply(normalMatrix(movie_length, dim, center, radius2, epsilon), Input2).reshape((center.shape[2],movie_length, Input.shape[1]))\n",
    "        #ParDeriv: each entry of matrix, reshaped as vector, is derivative of localized image wrt radius2\n",
    "        dR = np.einsum('ti, tk, ijk, tj, ktj -> tk', error, beta, phi, dTanh, ParDeriv)\n",
    "        radius2 += eta3*dR\n",
    "                                                                                                                      \n",
    "                                                                                                                \n",
    "    return(radius2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "localimg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 9, 1024)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newrad2 = InferRadius2(Input2, center, Output, radius2, phi, beta, eta3, epsilon, attStep)\n",
    "newrad2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferCenter(Input2, center, Output, radius2, phi, beta, eta4, epsilon, attStep):\n",
    "    \n",
    "    for step in range(attStep):\n",
    "    \n",
    "        localimg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1])) #calculate localized images and reshape\n",
    "        \n",
    "        predictions = np.einsum('tk,ijk,ktj -> ti', beta, phi, localimg)\n",
    "        error = Output - squash(predictions)\n",
    "        dTanh = squash_deriv(predictions)\n",
    "        centerPD = center_ParDeriv(movie_length, dim, center, radius2, epsilon).reshape((2,center.shape[2],movie_length, Input.shape[1]))\n",
    "        #ParDeriv: each entry of matrix, reshaped as vector, is derivative of localized image wrt radius2\n",
    "        dC = np.einsum('ti, tk, ijk, cktj -> ctk', error, beta, phi, centerPD)\n",
    "        center += eta4*dC\n",
    "                                                                                                                      \n",
    "                                                                                                                \n",
    "    return(center)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9, 15)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferCenter(Input2, center, Output, radius2, phi, beta, eta4, epsilon, attStep).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferBeta2(Input2, center, Output, radius2, phi ,beta, eta, epsilon, betaStep).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferBeta2(Input2, center, Output, radius2, phi ,beta, eta, epsilon, betaStep):\n",
    "    \n",
    "    for step in range(betaStep):\n",
    "        \n",
    "        localimg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1])) #calculate localized images and reshape\n",
    "        \n",
    "        predictions = np.einsum('tk,ijk,ktj -> ti', beta, phi, localimg)\n",
    "        error = Output - squash(predictions)\n",
    "        dTanh = squash_deriv(predictions)\n",
    "        dBeta = np.einsum('ti,ijk, tj, ktj -> tk', error, phi, dTanh, localimg) - sparsePen*np.sign(np.expand_dims(beta,axis=0))[0,:,:] - derivPen*timeDiff(np.expand_dims(beta,axis=0))[0,:,:]\n",
    "        #just a bit of reshaping magic above to make the functions work, since sparse/derivPen made for data with extra batch dim\n",
    "        beta += eta2*dBeta\n",
    "    \n",
    "    return(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferLocal(movie_seq, phi, sparsePen, derivPen, eta2, eta3, eta4, betaStep, attStep, numInference): \n",
    "    #now want to infer beta, center, and radii for a single movie sequence\n",
    "    #***RADII ARE SQUARED, to make computing the derivative easier***\n",
    "    #assume movie sequence is shape txn2, where t is length (movie_length) and n2 is product of dimensions of 2d image\n",
    "    \n",
    "\n",
    "    dim = np.sqrt(movie_seq.shape[1])\n",
    "    Input = movie_seq[:(movie_seq.shape[0]-1)]\n",
    "    Output = movie_seq[1:]\n",
    "    Input2 = Input.reshape((Input.shape[0],dim,dim)) #just reshape to make region of interest indices more natural\n",
    "    numInference2 = 10 #this is arbitrary. want to update each of beta, c, r a little\n",
    "    movie_length = Input2.shape[0]\n",
    "    \n",
    "    beta = np.zeros((Input2.shape[0],phi.shape[2])) #initialize beta at 0. beta is shape tk. movie_seq first dim is t\n",
    "    center = (dim/2)*np.ones((2,movie_length,phi.shape[2])) #shape 2tk (2 since center is 2-vector). initialize centers at center of image. each center is 2-vector\n",
    "    radius2 = (17**2)*np.ones((movie_length,phi.shape[2])) # shape tk.initialize radius2 at 17^2, so start off region of interest as entire image\n",
    "    \n",
    "    for bigStep in range(numInference):\n",
    "        beta = InferBeta2(Input2, center, Output, radius2, phi ,beta, eta, epsilon, betaStep)\n",
    "        radius2 = InferRadius2(Input2, center, Output, radius2, phi, beta, eta3, epsilon, attStep)\n",
    "        center = InferCenter(Input2, center, Output, radius2, phi, beta, eta4, epsilon)\n",
    "    \n",
    "    finalImg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1]))\n",
    "    final_predictions = np.einsum('tk,ijk,ktj -> ti', beta, phi, finalImg)\n",
    "    \n",
    "    fig = plt.figure(figsize = (20,6))\n",
    "    for i in range(movie_length):\n",
    "        for j in range(2):\n",
    "            fig.add_subplot(2,movie_length,movie_length*j+i+1)\n",
    "            if j == 0:\n",
    "                plt.imshow(Output[i].reshape((32,32)),cmap = \"Greys\")\n",
    "            else: \n",
    "                plt.imshow(final_predictions[i].reshape((32,32)),cmap=\"Greys\")\n",
    "    \n",
    "    return((beta,radius2,InferCenter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalImg = localImage(Input2, movie_length, dim, center, radius2, epsilon).reshape((center.shape[2],movie_length, Input.shape[1]))\n",
    "final_predictions = np.einsum('tk,ijk,ktj -> ti', beta, phi, finalImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,10))\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        fig.add_subplot(2,5,5*j+i+1)\n",
    "        plt.imshow(new[17*j,i].reshape((32,32)),cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20,6))\n",
    "for i in range(movie_length):\n",
    "    for j in range(2):\n",
    "        fig.add_subplot(2,movie_length,movie_length*j+i+1)\n",
    "        if j == 0:\n",
    "            plt.imshow(Output[i].reshape((32,32)),cmap = \"Greys\")\n",
    "        else: \n",
    "            plt.imshow(final_predictions[i].reshape((32,32)),cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radius2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1024)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
